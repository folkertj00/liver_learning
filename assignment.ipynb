{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SXpaKwwGe5x"
      },
      "source": [
        "# TM10007 Assignment template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiDn2Sk-VWqE",
        "outputId": "a1d2653f-9d48-42a7-8a01-110ffe911d8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ktml (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Run this to use from colab environment\n",
        "!pip install -q --upgrade git+https://github.com/jveenland/tm10007_ml.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "di79_xAuRIJ7"
      },
      "source": [
        "## Data loading and cleaning\n",
        "\n",
        "Below are functions to load the dataset of your choice. After that, it is all up to you to create and evaluate a classification method. Beware, there may be missing values in these datasets. Good luck!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NE_fTbKGe5z",
        "outputId": "ce7b4ba8-706d-4fb4-f344-942be7043198"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of samples: 186\n",
            "The number of columns: 494\n"
          ]
        }
      ],
      "source": [
        "# Data loading functions. Uncomment the one you want to use\n",
        "#from worcgist.load_data import load_data\n",
        "#from worclipo.load_data import load_data\n",
        "from worcliver.load_data import load_data\n",
        "#from ecg.load_data import load_data\n",
        "\n",
        "data = load_data()\n",
        "print(f'The number of samples: {len(data.index)}')\n",
        "print(f'The number of columns: {len(data.columns)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking for missing data**"
      ],
      "metadata": {
        "id": "kIJLESOytOyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A test if missing data is found correctly\n",
        "# print(data.iloc[5:8, 1:2])\n",
        "# data.iloc[5:8, 1:2] = None\n",
        "# print(data.iloc[5:8, 1:2])\n",
        "\n",
        "# Checking for missing data, returning specific column and row that is missing\n",
        "missing_data_total = data.isna().sum().sum()\n",
        "rows_with_missing = data[data.isna().any(axis=1)]\n",
        "print(f'Total amount of missing data: {missing_data_total}')\n",
        "for index, row in rows_with_missing.iterrows():\n",
        "    missing_columns = row[row.isna()].index\n",
        "    print(f\"Data missing in row: {index}, and column: {', '.join(missing_columns)}\")"
      ],
      "metadata": {
        "id": "Lp6mQ-RgtOkr",
        "outputId": "32efac7a-70f8-4e2d-f09f-802f18b0a96a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total amount of missing data: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imports**\n",
        "\n",
        "Import all modules necessary for this Jupyter Notebook\n"
      ],
      "metadata": {
        "id": "uydUOxjsfMP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn import datasets as ds\n",
        "\n",
        "#feature Selection\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Classifiers\n",
        "from sklearn import model_selection\n",
        "from sklearn import metrics\n",
        "from sklearn import feature_selection\n",
        "from sklearn import preprocessing\n",
        "from sklearn import neighbors\n",
        "from sklearn import svm\n",
        "from sklearn import decomposition\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Basic Classifiers\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n"
      ],
      "metadata": {
        "id": "bk7r6o7hfLVK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definitions applied in this Jupyter Notebook**"
      ],
      "metadata": {
        "id": "YDf4pxqtSyVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def colorplot(clf, ax, x, y, h=100, precomputer=None):\n",
        "    '''\n",
        "    Overlay the decision areas as colors in an axes.\n",
        "\n",
        "    Input:\n",
        "        clf: trained classifier\n",
        "        ax: axis to overlay color mesh on\n",
        "        x: feature on x-axis\n",
        "        y: feature on y-axis\n",
        "        h(optional): steps in the mesh\n",
        "    '''\n",
        "    # Create a meshgrid the size of the axis\n",
        "    xstep = (x.max() - x.min() ) / 20.0\n",
        "    ystep = (y.max() - y.min() ) / 20.0\n",
        "    x_min, x_max = x.min() - xstep, x.max() + xstep\n",
        "    y_min, y_max = y.min() - ystep, y.max() + ystep\n",
        "    h = max((x_max - x_min, y_max - y_min))/h\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "\n",
        "    features = np.c_[xx.ravel(), yy.ravel()]\n",
        "    if precomputer is not None:\n",
        "        if type(precomputer) is RBFSampler:\n",
        "            features = precomputer.transform(features)\n",
        "        elif precomputer is rbf_kernel:\n",
        "            features = rbf_kernel(features, X)\n",
        "\n",
        "    # Plot the decision boundary. For that, we will assign a color to each\n",
        "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "    if hasattr(clf, \"decision_function\"):\n",
        "        Z = clf.decision_function(features)\n",
        "    else:\n",
        "        Z = clf.predict_proba(features)\n",
        "    if len(Z.shape) > 1:\n",
        "        Z = Z[:, 1]\n",
        "\n",
        "    # Put the result into a color plot\n",
        "    cm = plt.cm.RdBu_r\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
        "    del xx, yy, x_min, x_max, y_min, y_max, Z, cm\n"
      ],
      "metadata": {
        "id": "6NgZNQSGSvoz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Visualization**"
      ],
      "metadata": {
        "id": "jLhtqHO7ew2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for data visualization\n"
      ],
      "metadata": {
        "id": "XDTeNvt6ewWq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PRE PROCESSING**\n",
        "\n",
        "Pre processing contains the following: ..."
      ],
      "metadata": {
        "id": "2IW8GCzuKt9V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GC1O47MQRIJ-",
        "outputId": "e9d341b2-8bfd-4ead-f501-98474743d188",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-fcfbe0828860>:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  y['label'] = y.replace({'benign': 0, 'malignant': 1})\n"
          ]
        }
      ],
      "source": [
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "\n",
        "# Pre processing\n",
        "#\n",
        "# Splitting\n",
        "df_data = pd.DataFrame(data)\n",
        "X = df_data.iloc[:, 1:]  # Feature set\n",
        "y = df_data.iloc[:, :1]  # Classification Benign/Malignant\n",
        "\n",
        "# Change classification (Benign/Malignant) to binary values\n",
        "y['label'] = y.replace({'benign': 0, 'malignant': 1})\n",
        "\n",
        "# Stratified K-Fold Split\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=40)\n",
        "\n",
        "# Splitting data in Test and Train set\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "    X_pre_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_pre_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "# Splitting data in Train and validation set\n",
        "for train_index, test_index in skf.split(X_pre_train, y_pre_train):\n",
        "    X_train, X_val = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "# print(y_train)\n",
        "# print(f'The number of samples: {len(y_train)}')\n",
        "# print(f'test{y_test}')\n",
        "# print(f'The number of samples: {len(y_test)}')\n",
        "# print(f'val{y_val}')\n",
        "# print(f'The number of samples: {len(y_val)}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scaling**"
      ],
      "metadata": {
        "id": "Y_qg4_ndMEFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling\n",
        "\n",
        "# # General packages\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn import datasets as ds\n",
        "# import seaborn\n",
        "\n",
        "# # Classifiers\n",
        "# from sklearn import model_selection\n",
        "# from sklearn import metrics\n",
        "# from sklearn import feature_selection\n",
        "# from sklearn import preprocessing\n",
        "# from sklearn import neighbors\n",
        "# from sklearn import svm\n",
        "\n",
        "# from sklearn import decomposition\n",
        "\n",
        "# Scale the data to be normal\n",
        "scaler = preprocessing.StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "print(X_train_scaled)\n",
        "\n",
        "y_train_scaled = y_train['label'].values\n",
        "\n",
        "# # Feature selection\n",
        "# # Create the RFE object and compute a cross-validated score.\n",
        "# svc = svm.SVC(kernel=\"linear\")\n",
        "\n",
        "# # classifications\n",
        "# rfecv = feature_selection.RFECV(\n",
        "#     estimator=svc, step=1,\n",
        "#     cv=model_selection.StratifiedKFold(4),\n",
        "#     scoring='roc_auc')\n",
        "# rfecv.fit(X, y)\n",
        "\n",
        "# # Plot number of features VS. cross-validation scores\n",
        "# plt.figure()\n",
        "# plt.xlabel(\"Number of features selected\")\n",
        "# plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
        "# plt.plot(range(1, len(rfecv.cv_results_[\"mean_test_score\"]) + 1), rfecv.cv_results_[\"mean_test_score\"])\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "nSHsBhpSKlBI",
        "outputId": "377e3320-23f3-46d9-e4f2-eb20f1aedbed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.94352879 -0.98601311  0.61787628 ... -0.32181343 -0.84659553\n",
            "   0.24217842]\n",
            " [ 0.24547317 -0.23475841  0.85830245 ...  0.13228887 -0.92318971\n",
            "   0.91654734]\n",
            " [ 0.28643264 -0.22331008 -0.19589509 ... -0.45398148  0.26261181\n",
            "  -0.31373541]\n",
            " ...\n",
            " [ 0.9785928  -0.73462397 -0.49196718 ... -0.54232872 -0.92318971\n",
            "  -0.55235085]\n",
            " [-0.5425542   2.55818432 -0.36556292 ... -0.48036377 -0.06022181\n",
            "   0.05117112]\n",
            " [ 0.23019558 -0.3527302  -0.07494348 ... -0.51519997 -0.46519492\n",
            "  -0.28835371]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-tIeB9l83IiI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Selection"
      ],
      "metadata": {
        "id": "SlkA0X15MlSV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Unsupervised\n"
      ],
      "metadata": {
        "id": "8nN-tW6G3GkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Drop Incomplete features\n",
        "X_selection = X.dropna(axis=1)\n",
        "#Drop high multicollinearity; multicollinearity means a strong correlation between different features, which might signal redundancy issues.\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "vif_scores = [variance_inflation_factor(X.values, feature)for feature in range(len(X.columns))]\n",
        "#Drop Zero or near-zero variance; Features that are (almost) constant provide little information to learn from and thus are irrelevant.\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "sel = VarianceThreshold(threshold=0.05)\n",
        "X_selection = sel.fit_transform(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "gTr9KIv23Ons",
        "outputId": "09d83338-d288-4567-8253-04c205ea4697"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/statsmodels/regression/linear_model.py:1782: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  return 1 - self.ssr/self.centered_tss\n",
            "/usr/local/lib/python3.10/dist-packages/statsmodels/regression/linear_model.py:1782: RuntimeWarning: divide by zero encountered in scalar divide\n",
            "  return 1 - self.ssr/self.centered_tss\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-5c218932ea93>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#Drop high multicollinearity; multicollinearity means a strong correlation between different features, which might signal redundancy issues.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutliers_influence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvariance_inflation_factor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mvif_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvariance_inflation_factor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#Drop Zero or near-zero variance; Features that are (almost) constant provide little information to learn from and thus are irrelevant.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVarianceThreshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-5c218932ea93>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#Drop high multicollinearity; multicollinearity means a strong correlation between different features, which might signal redundancy issues.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutliers_influence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvariance_inflation_factor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mvif_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvariance_inflation_factor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#Drop Zero or near-zero variance; Features that are (almost) constant provide little information to learn from and thus are irrelevant.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVarianceThreshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/statsmodels/stats/outliers_influence.py\u001b[0m in \u001b[0;36mvariance_inflation_factor\u001b[0;34m(exog, exog_idx)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_vars\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mexog_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mx_noti\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0mr_squared_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOLS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_noti\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsquared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m     \u001b[0mvif\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mr_squared_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvif\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/statsmodels/regression/linear_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, method, cov_type, cov_kwds, use_t, **kwargs)\u001b[0m\n\u001b[1;32m    333\u001b[0m                     hasattr(self, 'rank')):\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpinv_wexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msingular_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpinv_extended\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwexog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m                 self.normalized_cov_params = np.dot(\n\u001b[1;32m    337\u001b[0m                     self.pinv_wexog, np.transpose(self.pinv_wexog))\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/statsmodels/tools/tools.py\u001b[0m in \u001b[0;36mpinv_extended\u001b[0;34m(x, rcond)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconjugate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m     \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m     \u001b[0ms_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, hermitian)\u001b[0m\n\u001b[1;32m   1679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1680\u001b[0m         \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'D->DdD'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'd->ddd'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1681\u001b[0;31m         \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1682\u001b[0m         \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1683\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_realType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Supervised feature selection"
      ],
      "metadata": {
        "id": "kGjayRaq3SH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Embedded methods"
      ],
      "metadata": {
        "id": "M6D_1V0Bj6lN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**L1 Regularization LASSO \\(least absolute shrinkage and selection operator)**"
      ],
      "metadata": {
        "id": "9qwLwt4GMvaz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Applies a penalty to the absolute size of the features, forcing some of them to be exactly zero. Resulting in a subset of the features to contribute in the algorithm by eliminating irrelevant features from the model indicating that these features are not contributing significantly to the model's predictive power.*"
      ],
      "metadata": {
        "id": "ddNO2eAKQYpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###This method now prints the features selected with the lasso method to predict maligne of benigne\n",
        "\n",
        "#Fit the model to training data\n",
        "lasso = Lasso(alpha=0.01)  # 'alpha' is the regularization (= prevent overfitting) strength, higher values indicate stronger regularization\n",
        "lasso.fit(X_train_scaled, y_train_scaled)\n",
        "\n",
        "#Print selected features to be non-zero\n",
        "selected_features_lasso = X.columns[lasso.coef_ != 0] #selects the features used in the model and thus relevant for predictions\n",
        "print(\"Selected Features:\", selected_features_lasso)\n",
        "\n",
        "# Create an Array containing only the selected features from the train data\n",
        "selected_train_features_lasso_df = pd.DataFrame(X_train_scaled[:, lasso.coef_ != 0], columns=selected_features_lasso)\n",
        "X_train_lasso = X_train_scaled[:, lasso.coef_ != 0]\n",
        "y_train_scaled_df = pd.DataFrame(y_train_list, index=selected_train_features_lasso_df.index, columns=['Tumor'])\n",
        "y_train_lasso = y_train_scaled_df.values\n",
        "\n",
        "print(f'y train lasso: {y_train_lasso}')\n",
        "print(f'x train lasso: {X_train_lasso}')\n",
        "\n",
        "# Create an Array containing only the selected features from the validation data\n",
        "selected_val_features_lasso_df = pd.DataFrame(X_val_scaled[:, lasso.coef_ != 0], columns=selected_features_lasso)\n",
        "X_val_lasso = X_val_scaled[:, lasso.coef_ != 0]\n",
        "y_val_scaled_df = pd.DataFrame(y_val_list, index=selected_val_features_lasso_df.index, columns=['Tumor'])\n",
        "y_val_lasso = y_val_scaled_df.values\n",
        "\n",
        "print(f'y val lasso: {y_val_lasso}')\n",
        "print(f'x val lasso: {X_val_lasso}')\n",
        "\n",
        "# Concatenate selected features DataFrame with y_train\n",
        "selected_train_data = pd.concat([selected_train_features_lasso_df, y_train_scaled_df], axis=1)\n",
        "\n",
        "# Now selected_train_data contains the values of the selected features in separate columns\n",
        "print(selected_train_data.head())  # Display the first few rows of the selected train data\n",
        "\n",
        "#Evaluate model performance\n",
        "lasso_score = lasso.score(X_train_scaled, y_train)\n",
        "print(\"Lasso Model Score:\", lasso_score)\n",
        "\n",
        "#Evaluate model performance\n",
        "lasso_score = lasso.score(X_val_scaled, y_val_list)\n",
        "print(\"Validation Lasso Model Score:\",lasso_score)"
      ],
      "metadata": {
        "id": "GHdg_4pFOXsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**L2 Ridge Regularizaiton**\n"
      ],
      "metadata": {
        "id": "OGqdkddiW8ci"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Adds a penalty term proportional to the square of the coefficients,encourages smaller but non-zero coefficients for all features. Helps to reduce the impact of multicollinearity and stabilize the model meaning that when two features are highly correlated then one is given an smaller coefficient to prevent instability of the model estimates.*"
      ],
      "metadata": {
        "id": "lsNGS3y_XEpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###This method now prints the features selected with the lasso method to predict maligne of benigne\n",
        "\n",
        "# Fit the Ridge regression model to training data\n",
        "ridge = Ridge(alpha=0.01)  # 'alpha' is the regularization strength, higher values indicate stronger regularization\n",
        "ridge.fit(X_train_scaled, y_train_scaled)\n",
        "\n",
        "#Print selected features (all features are used in Ridge regression)\n",
        "selected_features_ridge = X.columns\n",
        "print(\"Selected Features:\", selected_features_ridge)\n",
        "\n",
        "# Create a DataFrame containing all features from the test data\n",
        "selected_train_features_ridge_df = pd.DataFrame(X_train_scaled, columns=selected_features_ridge)\n",
        "y_train_scaled_df = pd.DataFrame(y_train_scaled, index=selected_train_features_ridge_df.index, columns=['Tumor'])\n",
        "\n",
        "# Concatenate selected features DataFrame with y_train\n",
        "selected_train_data = pd.concat([selected_train_features_ridge_df, y_train_scaled_df], axis=1)\n",
        "\n",
        "# Now selected_train_data contains the values of all features along with the target variable\n",
        "print(selected_train_data.head())  # Display the first few rows of the selected train data\n",
        "\n",
        "# Visualize Ridge coefficients\n",
        "def plot_coefficients(model, feature_names, title):\n",
        "    coef = model.coef_.flatten()  # Coefficients need to be flattened for visualization\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(range(len(feature_names)), coef, align='center')\n",
        "    plt.xticks(range(len(feature_names)), feature_names, rotation=90)\n",
        "    plt.xlabel('Feature')\n",
        "    plt.ylabel('Coefficient Value')\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "plot_coefficients(ridge, X.columns, 'Ridge Coefficients')\n",
        "\n",
        "# Evaluate model performance\n",
        "ridge_score = ridge.score(X_train_scaled, y_train)\n",
        "print(\"Ridge Model Score:\", ridge_score)\n"
      ],
      "metadata": {
        "id": "3VbZQXZ3XiHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Wrapper methods (greedy selection)"
      ],
      "metadata": {
        "id": "GrGeeQPKkBkU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Forward selection**"
      ],
      "metadata": {
        "id": "QcBxJ_uTkHBU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*In forward selection, we start with a null model and then start fitting the model with each individual feature one at a time and select the feature with the minimum p-value. Now fit a model with two features by trying combinations of the earlier selected feature with all other remaining features. Again select the feature with the minimum p-value. Now fit a model with three features by trying combinations of two previously selected features with other remaining features. Repeat this process until we have a set of selected features with a p-value of individual features less than the significance level.*"
      ],
      "metadata": {
        "id": "aBcx3Eacl2M2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "# Sequential Forward Selection(sfs)\n",
        "sfs = SFS(RandomForestRegressor(),\n",
        "          k_features=(1,6), #Selecting the number of best features???? How to determine; or is this hyperparameter tuning?\n",
        "          forward=True,\n",
        "          floating=False,\n",
        "          scoring = 'r2', #default\n",
        "          cv = 0)\n",
        "sfs.fit(X_train_scaled, y_train_scaled)\n",
        "sfs.k_feature_names_     # to get the final set of features\n",
        "\n",
        "# Get the performance metric (R-squared) for each number of features\n",
        "metric_dict = sfs.get_metric_dict()\n",
        "\n",
        "# Extracting the optimal number of features\n",
        "optimal_num_features = sfs.k_feature_idx_\n",
        "\n",
        "# Extracting the performance score (R-squared) for the optimal number of features\n",
        "r2_optimal = metric_dict[optimal_num_features]['avg_score']\n",
        "\n",
        "# Print the performance score\n",
        "print(\"Performance Score (R-squared) for the optimal number of features:\", r2_optimal)\n",
        "\n",
        "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
        "fig1 = plot_sfs(sfs.get_metric_dict(), kind='std_dev')\n",
        "plt.title('Sequential Forward Selection (w. StdErr)')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Extracting the optimal number of features\n",
        "optimal_num_features = sfs.k_feature_idx_\n",
        "print(optimal_num_features)\n",
        "\n",
        "# Extracting the names of the selected features\n",
        "selected_feature_names = list(X.columns[list(optimal_num_features)])\n",
        "\n",
        "# Creating a dataframe with the selected features\n",
        "selected_features_df = pd.DataFrame({'Feature': selected_feature_names})\n",
        "print(\"Selected features:\")\n",
        "print(selected_features_df)\n",
        "\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "\n",
        "# Fit the Linear Regression model with selected features\n",
        "selected_features = X_train_scaled[:, optimal_num_features]\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(selected_features, y_train_scaled)\n",
        "\n",
        "# Cross-validation predictions\n",
        "cv_predictions = cross_val_predict(lr_model, selected_features, y_train_scaled, cv=5)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "r2 = r2_score(y_train_scaled, cv_predictions)\n",
        "mae = mean_absolute_error(y_train_scaled, cv_predictions)\n",
        "mse = mean_squared_error(y_train_scaled, cv_predictions)\n",
        "\n",
        "# Print performance metrics\n",
        "print(\"Performance Metrics:\")\n",
        "print(\"R-squared:\", r2) #Predicts the fit of the model to the data\n",
        "print(\"Mean Absolute Error:\", mae)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "\n",
        "###Validation\n",
        "# Transform the validation set using the same preprocessing steps as the training set\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "# Select the same features that were selected during training\n",
        "selected_features_val = X_val_scaled[:, optimal_num_features]\n",
        "\n",
        "# Make predictions on the validation set\n",
        "val_predictions = rf_model.predict(selected_features_val)\n",
        "\n",
        "# Calculate evaluation metrics on the validation set\n",
        "val_mae = mean_absolute_error(y_val_scaled, val_predictions)\n",
        "val_mse = mean_squared_error(y_val_scaled, val_predictions)\n",
        "val_r2 = r2_score(y_val_scaled, val_predictions)\n",
        "\n",
        "# Print validation performance metrics\n",
        "print(\"Validation Performance Metrics:\")\n",
        "print(\"Mean Absolute Error:\", val_mae)\n",
        "print(\"Mean Squared Error:\", val_mse)\n",
        "print(\"R-squared:\", val_r2)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cQ-mxyyOkGch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Backward Elimination**"
      ],
      "metadata": {
        "id": "Rscc4UQIorwQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*In backward elimination, we start with the full model (including all the independent variables) and then remove the insignificant feature with the highest p-value(> significance level). This process repeats again and again until we have the final set of significant features.*"
      ],
      "metadata": {
        "id": "3RIGHXJ0o0od"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Sequential backward selection(sbs)\n",
        "sbs = SFS(LinearRegression(),\n",
        "         k_features=(1,X_train_scaled.shape[1]),\n",
        "         forward=False,\n",
        "         floating=False,\n",
        "         cv=0)\n",
        "sbs.fit(X_train_scaled, y_train_scaled)\n",
        "sbs.k_feature_names_\n",
        "\n",
        "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
        "fig2 = plot_sfs(sbs.get_metric_dict(), kind='std_dev')\n",
        "plt.title('Sequential backward elimination (w. StdErr)')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Extracting the optimal number of features\n",
        "optimal_num_features = sbs.k_feature_idx_\n",
        "print(optimal_num_features)\n",
        "\n",
        "# Extracting the names of the selected features\n",
        "selected_feature_names = list(X.columns[list(optimal_num_features)])\n",
        "\n",
        "# Creating a dataframe with the selected features\n",
        "selected_features_df = pd.DataFrame({'Feature': selected_feature_names})\n",
        "print(\"Selected features:\")\n",
        "print(selected_features_df)\n",
        "\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "\n",
        "# Fit the Linear Regression model with selected features\n",
        "selected_features = X_train_scaled[:, optimal_num_features]\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(selected_features, y_train_scaled)\n",
        "\n",
        "# Cross-validation predictions\n",
        "cv_predictions = cross_val_predict(lr_model, selected_features, y_train_scaled, cv=5)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "r2 = r2_score(y_train_scaled, cv_predictions)\n",
        "mae = mean_absolute_error(y_train_scaled, cv_predictions)\n",
        "mse = mean_squared_error(y_train_scaled, cv_predictions)\n",
        "\n",
        "# Print performance metrics\n",
        "print(\"Performance Metrics:\")\n",
        "print(\"R-squared:\", r2) #Predicts the fit of the model to the data\n",
        "print(\"Mean Absolute Error:\", mae)\n",
        "print(\"Mean Squared Error:\", mse)"
      ],
      "metadata": {
        "id": "pfU-SUa9rMnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bi-directional elimination(Step-wise Selection)**"
      ],
      "metadata": {
        "id": "tIUYH5yPumWr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*It is similar to forward selection but the difference is while adding a new feature it also checks the significance of already added features and if it finds any of the already selected features insignificant then it simply removes that particular feature through backward elimination. Hence, It is a combination of forward selection and backward elimination.*"
      ],
      "metadata": {
        "id": "Y4MUTa0puyi1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sequential Forward Floating Selection(sffs)\n",
        "sffs = SFS(LinearRegression(),\n",
        "         k_features=(1,X_train_scaled.shape[1]),\n",
        "         forward=True,\n",
        "         floating=True, #strategy where features are added and removed iteratively; the algorithm checks if removing or adding another feature improves the performance further.\n",
        "         cv=0)\n",
        "sffs.fit(X, y)\n",
        "sffs.k_feature_names_\n",
        "\n",
        "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
        "fig3 = plot_sfs(sffs.get_metric_dict(), kind='std_dev')\n",
        "plt.title('Sequential Step-wise seleciton (w. StdErr)')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Extracting the optimal number of features\n",
        "optimal_num_features = sffs.k_feature_idx_\n",
        "print(optimal_num_features)\n",
        "\n",
        "# Extracting the names of the selected features\n",
        "selected_feature_names = list(X.columns[list(optimal_num_features)])\n",
        "\n",
        "# Creating a dataframe with the selected features\n",
        "selected_features_df = pd.DataFrame({'Feature': selected_feature_names})\n",
        "print(\"Selected features:\")\n",
        "print(selected_features_df)\n",
        "\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "\n",
        "# Fit the Linear Regression model with selected features\n",
        "selected_features = X_train_scaled[:, optimal_num_features]\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(selected_features, y_train_scaled)\n",
        "\n",
        "# Cross-validation predictions\n",
        "cv_predictions = cross_val_predict(lr_model, selected_features, y_train_scaled, cv=5)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "r2 = r2_score(y_train_scaled, cv_predictions)\n",
        "mae = mean_absolute_error(y_train_scaled, cv_predictions)\n",
        "mse = mean_squared_error(y_train_scaled, cv_predictions)\n",
        "\n",
        "# Print performance metrics\n",
        "print(\"Performance Metrics:\")\n",
        "print(\"R-squared:\", r2) #Predicts the fit of the model to the data\n",
        "print(\"Mean Absolute Error:\", mae)\n",
        "print(\"Mean Squared Error:\", mse)"
      ],
      "metadata": {
        "id": "ikhHDqbDu3In"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "svc = SVC(kernel=\"linear\")\n",
        "rfe = RFE(svc, n_features_to_select=3)\n",
        "rfe.fit(X, y)\n",
        "X_selection = rfe.transform(X)\n",
        "\n"
      ],
      "metadata": {
        "id": "npVT5pECSK20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Filter methods"
      ],
      "metadata": {
        "id": "bFIlmBztwI5-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature extraction"
      ],
      "metadata": {
        "id": "xWC3LaVQ6Wdm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PCA"
      ],
      "metadata": {
        "id": "9eTFBN6P6aYZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*In PCA we try to create n composite features which can best represent the information contained in the number of features of our dataset. These n features are not original features, but developed as a combination of different already existing features.*"
      ],
      "metadata": {
        "id": "WT6-ZJ35h0Sf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform a PCA\n",
        "pca = decomposition.PCA(n_components=2)\n",
        "pca.fit(X_train_scaled)\n",
        "X_train_pca = pca.transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "y_train_list = y_train['label'].tolist()\n",
        "print(y_train_list)\n",
        "\n",
        "###########\n",
        "\n",
        "# Determine the number of components to keep\n",
        "cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
        "num_components = np.argmax(cumulative_variance_ratio >= 0.95) + 1\n",
        "\n",
        "# Select the top principal components\n",
        "pca = PCA(n_components=num_components)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Print the number of selected components\n",
        "print(\"Number of selected components:\", num_components)"
      ],
      "metadata": {
        "id": "A39pkwuY6fOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classifiers"
      ],
      "metadata": {
        "id": "yUkgqj26ejAk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fvaLpTPd6ZRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Basic Classifiers**"
      ],
      "metadata": {
        "id": "KWAoHyyrP5ki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # General packages\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn import datasets as ds\n",
        "# from sklearn import metrics\n",
        "\n",
        "# # Metrics\n",
        "# from sklearn.metrics import confusion_matrix\n",
        "# from sklearn.metrics import mean_absolute_error\n",
        "# from sklearn.metrics import r2_score\n",
        "\n",
        "# # Basic Classifiers\n",
        "# from sklearn.naive_bayes import GaussianNB\n",
        "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "# from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.linear_model import SGDClassifier\n",
        "# from sklearn.tree import DecisionTreeClassifier\n",
        "# from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Linear classifier\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "ax = fig.add_subplot(111)\n",
        "ax.set_title(\"Two informative features, one cluster per class\",\n",
        "             fontsize='small')\n",
        "ax.scatter(X_train_pca[:, 0], X_train_pca[:, 1], marker='o', c=y_train_list,\n",
        "           s=25, edgecolor='k', cmap=plt.cm.Paired)\n",
        "lda = LinearDiscriminantAnalysis()\n",
        "lda = lda.fit(X_train_pca, y_train_list)\n",
        "y_pred = lda.predict(X_train_pca)\n",
        "colorplot(lda, ax, X_train_pca[:, 0], X_train_pca[:, 1])\n",
        "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_train_pca.shape[0], (y_train_list != y_pred).sum()))\n"
      ],
      "metadata": {
        "id": "w7hIfsfiP9Q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Basic classifiers: Gaussian, LDA, QDA, Logistic regression, SDG, KNN & Decision Tree**"
      ],
      "metadata": {
        "id": "3DSKEvQgYULm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot several basic classifiers and plot the result. Define misclasses\n",
        "clsfs = [LinearDiscriminantAnalysis(),QuadraticDiscriminantAnalysis(),GaussianNB(),\n",
        "         LogisticRegression(),SGDClassifier(),KNeighborsClassifier(),DecisionTreeClassifier()]\n",
        "Xs = X_train_pca\n",
        "Ys = y_train_list\n",
        "clfs_fit = list()\n",
        "\n",
        "# First make a plot without classifiers:\n",
        "fig = plt.figure(figsize=(21,3*len(clsfs)))\n",
        "num = 0  # Iteration number for the subplots\n",
        "\n",
        "for num in range(0,7):\n",
        "    ax = fig.add_subplot(21, 3, num + 1)\n",
        "    ax.scatter(Xs[:, 0], Xs[:, 1], marker='o', c=Ys,\n",
        "            s=25, edgecolor='k', cmap=plt.cm.Paired)\n",
        "\n",
        "# Fit the classifiers and add them to the plot\n",
        "num=0\n",
        "Xt=list()\n",
        "Yt=list()\n",
        "for clf in clsfs:\n",
        "    # Fit classifier\n",
        "    clf.fit(Xs,Ys)\n",
        "    y_pred=clf.predict(Xs)\n",
        "    # Predict labels using fitted classifier\n",
        "\n",
        "    # Make scatterplot of features\n",
        "    ax = fig.add_subplot(3, 3, num + 1)\n",
        "    ax.scatter(Xs[:, 0], Xs[:, 1], marker='o', c=Ys,\n",
        "            s=25, edgecolor='k', cmap=plt.cm.Paired)\n",
        "    colorplot(clf, ax, Xs[:,0], Xs[:,1])\n",
        "    # Add overlay through colorplot function\n",
        "    t=(f\"{clf}, Misclass: %d / %d\" % ((Ys!=y_pred).sum(), Xs.shape[0]))\n",
        "    ax.set_title(t)\n",
        "    num+=1\n",
        "\n",
        "    clfs_fit.append(clf)\n",
        "    Xt.append(Xs)\n",
        "    Yt.append(Ys)\n"
      ],
      "metadata": {
        "id": "rZGT6b7nYUAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**KNN**"
      ],
      "metadata": {
        "id": "oksvSMtzPmu5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit kNN\n",
        "knn = neighbors.KNeighborsClassifier(n_neighbors=15)\n",
        "knn.fit(X_train_pca, y_train)\n",
        "score_train = knn.score(X_train_pca, y_train)\n",
        "score_test = knn.score(X_test_pca, y_test)\n",
        "\n",
        "# Print result\n",
        "print(f\"Training result: {score_train}\")\n",
        "print(f\"Test result: {score_test}\")"
      ],
      "metadata": {
        "id": "SFMvt4EuPjUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the metrics on the used classifiers"
      ],
      "metadata": {
        "id": "Cxh8lZpvtwSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "df_data = pd.DataFrame(data)\n",
        "# X1 = df_data.iloc[:, 1:]        # Feature set\n",
        "y_truth = df_data.iloc[:, :1]   # Classification Benign/Malignant\n",
        "# gnb = GaussianNB()\n",
        "# gnb = gnb.fit(X1, y_truth)\n",
        "# y_pred = gnb.predict(X1)\n",
        "all_results={}\n",
        "\n",
        "for clf, X1, Y1 in zip(clfs_fit, Xt, Yt):\n",
        "    y_pred=clf.predict(X1)\n",
        "\n",
        "    if hasattr(clf, 'predict_proba'):\n",
        "    # The first column gives the probability for class = 0, so we take\n",
        "    # the second which gives the probability class = 1:\n",
        "        y_score = clf.predict_proba(X1)[:, 1]\n",
        "    else:\n",
        "        y_score = y_pred\n",
        "\n",
        "    # The hasattr function checks whether an object, function or package has\n",
        "    # a certain attribute. This attribute can be a subfunction, or again an\n",
        "    # object or function, but also things like scalars or strings.\n",
        "\n",
        "    auc_score=metrics.roc_auc_score(Y1, y_score)\n",
        "    accuracy=metrics.accuracy_score(Y1, y_pred)\n",
        "    F1=metrics.f1_score(Y1,y_pred)\n",
        "    precision=metrics.precision_score(Y1,y_pred)\n",
        "    recall=metrics.recall_score(Y1, y_pred)\n",
        "    # accuracy, AUC, f1score, precision, recall\n",
        "    result = [auc_score,accuracy,F1,precision,recall]\n",
        "    all_results[clf]=result\n",
        "\n",
        "    # Calculate ROC curve\n",
        "    fpr, tpr, thresholds = roc_curve(Y1, y_score)\n",
        "\n",
        "    # Plot ROC curve\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (AUC = %0.2f)' % auc_score)\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(f'ROC Curve {clf}')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "results_pd = pd.DataFrame(all_results)\n",
        "results_pd.index = ['auc','accuracy','F1','precision','recall']\n",
        "results_pd\n",
        "## transpose possible if we want the axes different\n",
        "# results_trans=results_pd.transpose()\n",
        "# results_trans"
      ],
      "metadata": {
        "id": "SfqywSa8t1AC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "assignment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}